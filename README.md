## ğŸš€ Run on Google Colab
<a href="https://colab.research.google.com/drive/1PkmFmONni69H041tQiuRz2KajyN8e1rC">
    <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open in Colab"/>
</a>

# Characterizing and Evaluating Adversarial Examples in Handwritten Signature Verification

Handwritten signature verification models are increasingly used in digital documentation, banking, and biometric authentication. 
However, deep learning models are vulnerable to **adversarial attacks** â€” tiny intentionally crafted perturbations that can trick a trained classifier into labeling forged signatures as genuine.

This project investigates:  
âœ”ï¸ Model accuracy against adversarial examples  
âœ”ï¸ Impact of FGSM perturbations on model predictions  
âœ”ï¸ How **adversarial training** improves robustness  

---

## ğŸ“Œ Problem Statement
Traditional manual signature verification is slow, non-scalable, and prone to human error.  
Deep learning models such as CNNs solve this â€” but they:

- Misclassify adversarial signatures
- Are vulnerable to pixel-level perturbations
- Fail silently in real biometric systems

### This project shows:
- How adversarial noise fools a ResNet50 classifier
- How adversarial training increases resilience

---

## ğŸ§  Approach

### 1ï¸âƒ£ Dataset â€” CEDAR Signature Dataset
- 55 writers
- 24 genuine + 24 forged per writer
- 2,640 scanned grayscale images
- Benchmark dataset for offline signature verification

> Dataset Source: https://cedar.buffalo.edu/signature/

---

### 2ï¸âƒ£ Preprocessing Pipeline
To standardize input to ResNet50:

- Grayscale conversion
- Resize to **224Ã—224**
- **OTSU thresholding** (binarization)
- Bitwise inversion
- Convert to 3-channel RGB
- Normalize with `preprocess_input`

---

### 3ï¸âƒ£ Base Model â€” ResNet50 (Transfer Learning)
ResNet50 pretrained on ImageNet (frozen)
â†’ GlobalAveragePooling2D
â†’ Dropout(0.5)
â†’ Dense(1, sigmoid)

âœ”ï¸ Binary classification (genuine vs forged)  
âœ”ï¸ Dropout reduces overfitting  
âœ”ï¸ Fine-tuning last 20 layers improves accuracy  

---

## âš”ï¸ Adversarial Attack â€” Fast Gradient Sign Method (FGSM)

FGSM creates adversarial sample:
x_adv = x + Îµ Â· sign(âˆ‡loss)
Where:
- `Îµ` = perturbation strength
- `sign(gradient)` = direction to increase loss

Produces **imperceptible noise** that changes model predictions.

---

## ğŸ›¡ï¸ Adversarial Training
The model is retrained using mixed batches:

- Clean inputs
- FGSM perturbed inputs

> Result: Model learns **robust features** and resists attacks.

---

## ğŸ“Š Evaluation Metrics

- Accuracy
- Precision / Recall
- F1 Score
- ROCâ€“AUC
- Misclassification patterns
- Visual inspection (original vs adversarial)

---

# ğŸ“‚ Project Structure  
Signature-Adversarial-Verification/  
â”‚
â”œâ”€â”€ src/ # Source code  
â”‚ â””â”€â”€ signature_verification_adversarial.py  
â”‚
â”œâ”€â”€ docs/ # Documentation  
â”‚ â””â”€â”€ Project_Report.pdf  
â”‚
â”œâ”€â”€ assets/ # Images / plots / sample outputs    
â”‚
â”œâ”€â”€ requirements.txt  
â”œâ”€â”€ LICENSE  
â””â”€â”€ README.md  
---  

# â–¶ï¸ Running Locally

1ï¸âƒ£ Install dependencies  
pip install -r requirements.txt

2ï¸âƒ£ Run the projectpython
src/signature_verification_adversarial.py  
Note: This script was developed in Google Colab.  
Local paths may need modification depending on your environment.  

ğŸ“„ Full Project Report
The complete documentation with diagrams and experimental results is provided here:  
docs/Project_Report.pdf

ğŸ§ª Future Improvements
Evaluate stronger adversarial attacks:  
PGD  
DeepFool  
CW  
Train Siamese or Triplet networks for signature embeddings  
Add visual explainability (Grad-CAM)  
Deploy as an API for real-time verification  

ğŸ‘¥ Contributors
Challapalli Sathwik
Talasila Revanth
B Sanjeev Roy

ğŸ“š References
Goodfellow et al., Explaining and Harnessing Adversarial Examples
He et al., Deep Residual Learning for Image Recognition (ResNet)
Simonyan & Zisserman, Very Deep Convolutional Networks
CEDAR Signature Dataset â€” https://cedar.buffalo.edu/signature/
TensorFlow FGSM tutorial â€” https://www.tensorflow.org/tutorials/generative/adversarial_fgsm
